{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import TensorBoard\n",
    "from gensim.parsing.preprocessing import strip_short, remove_stopwords, preprocess_string, strip_tags, strip_punctuation\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_pickle('dad_jokes.pkl')\n",
    "df['joke_text_raw'] = df['title'] + \" \" + df['selftext']\n",
    "df['joke_text_process'] = df['joke_text_raw'].str.lower().apply(strip_punctuation).apply(strip_tags)\n",
    "df['joke_text_process'] = df['joke_text_process'].replace(r'\\n',' ', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke_text_process'].head()\n",
    "df = df.drop_duplicates(subset = 'joke_text_process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(num_words = 20000)\n",
    "df['length']= df['joke_text_process'].str.split().apply(len)\n",
    "t.fit_on_texts(df['joke_text_process'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= t.texts_to_sequences(df['joke_text_process'])\n",
    "X[0]\n",
    "word_index = t.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8391\n",
       "2    5769\n",
       "3    1338\n",
       "Name: score_bucket, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.score.describe()\n",
    "df['score_bucket'] = np.where(df['score']<10,1,np.where(df['score']<100,2,3))\n",
    "df.score_bucket.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequence.pad_sequences(X, maxlen=40)\n",
    "y = pandas.get_dummies(df['score_bucket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .2, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# set up embedding\n",
    "embeddings_index = {}\n",
    "f = open('/Users/dweiss89/ds/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=40,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 12398 samples, validate on 3100 samples\n",
      "Epoch 1/10\n",
      "12398/12398 [==============================] - 49s 4ms/step - loss: 0.9167 - acc: 0.5311 - val_loss: 0.9097 - val_acc: 0.5400\n",
      "Epoch 2/10\n",
      "12398/12398 [==============================] - 45s 4ms/step - loss: 0.8999 - acc: 0.5407 - val_loss: 0.9082 - val_acc: 0.5345\n",
      "Epoch 3/10\n",
      "12398/12398 [==============================] - 42s 3ms/step - loss: 0.8854 - acc: 0.5497 - val_loss: 0.9189 - val_acc: 0.5452\n",
      "Epoch 4/10\n",
      "12398/12398 [==============================] - 44s 4ms/step - loss: 0.8660 - acc: 0.5682 - val_loss: 0.9236 - val_acc: 0.5252\n",
      "Epoch 5/10\n",
      "12398/12398 [==============================] - 41s 3ms/step - loss: 0.8362 - acc: 0.5916 - val_loss: 0.9328 - val_acc: 0.5274\n",
      "Epoch 6/10\n",
      "12398/12398 [==============================] - 43s 3ms/step - loss: 0.7945 - acc: 0.6241 - val_loss: 0.9751 - val_acc: 0.5335\n",
      "Epoch 7/10\n",
      "12398/12398 [==============================] - 43s 3ms/step - loss: 0.7437 - acc: 0.6606 - val_loss: 0.9936 - val_acc: 0.5171\n",
      "Epoch 8/10\n",
      "12398/12398 [==============================] - 46s 4ms/step - loss: 0.6828 - acc: 0.6987 - val_loss: 1.0344 - val_acc: 0.5200\n",
      "Epoch 9/10\n",
      "12398/12398 [==============================] - 45s 4ms/step - loss: 0.6126 - acc: 0.7335 - val_loss: 1.1407 - val_acc: 0.5216\n",
      "Epoch 10/10\n",
      "12398/12398 [==============================] - 42s 3ms/step - loss: 0.5390 - acc: 0.7763 - val_loss: 1.1781 - val_acc: 0.5187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a29648400>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"best_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(3,activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics = ['acc'])\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5742615 , 0.30727646, 0.11846203],\n",
       "       [0.56965834, 0.35712183, 0.07321991]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tim = \"you're American when you go into the bathroom, and you're American when you come out, but do you know what you are while you're in there? European\"\n",
    "ben = \"What did the buffalo say to his son when he left for college? Bison\"\n",
    "li = [tim,ben]\n",
    "tokens = sequence.pad_sequences(t.texts_to_sequences(li), maxlen=40)\n",
    "tokens\n",
    "model.predict(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
